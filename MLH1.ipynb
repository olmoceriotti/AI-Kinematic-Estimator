{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrzCvNCx2zkh"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK2VOFt5RjTZ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install keras-tuner --upgrade\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\n",
        "from kerastuner.tuners import Hyperband\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError, Huber, LogCosh\n",
        "from tensorflow.keras.layers import Dense, Dropout, Lambda, InputLayer\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.math import sin, cos\n",
        "import wandb\n",
        "from wandb.integration.keras import WandbCallback\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV_b6MA-93gD"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDjb6zOy0ZZe"
      },
      "outputs": [],
      "source": [
        "seed = 2193258"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igcKGEgh79Rq"
      },
      "outputs": [],
      "source": [
        "def start_log_wandb(lr, model_name, dataset, epochs, opt, loss, metrics, batch_size, split, hidden_layers, units_per_layer, dropout, full_output, noise, noise_percent):\n",
        "  wandb.init(\n",
        "    project=\"MLH1\",\n",
        "\n",
        "    config={\n",
        "    \"learning_rate\": lr,\n",
        "    \"architecture\": model_name,\n",
        "    \"dataset\": dataset,\n",
        "    \"epochs\": epochs,\n",
        "    \"optimizer\": opt,\n",
        "    \"loss\": loss,\n",
        "    \"metrics\": metrics,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"split\": split,\n",
        "    \"hidden_layers\": hidden_layers,\n",
        "    \"units_per_layer\": units_per_layer,\n",
        "    \"dropout\": dropout,\n",
        "    \"full_output\": full_output,\n",
        "    \"noise\": noise,\n",
        "    \"noise_percent\": noise_percent\n",
        "    }\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKq3W3SPCyKI"
      },
      "outputs": [],
      "source": [
        "if tf.config.list_physical_devices('GPU'):\n",
        "  print(\"GPU is available.\")\n",
        "  tf.config.set_visible_devices([], 'GPU')\n",
        "else:\n",
        "  print(\"GPU is not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FyNY9jB22fJ"
      },
      "source": [
        "## Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3bBhsjE7VQ_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_path = '/content/drive/MyDrive/MLH1/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BV6D-vESWOR"
      },
      "outputs": [],
      "source": [
        "d2_file = base_path + 'datasetR2.csv'\n",
        "d3_file = base_path + 'datasetR3.csv'\n",
        "d5_file = base_path + 'datasetR5.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV5TwGhp3dBI"
      },
      "source": [
        "## Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk-vO8z4Didx"
      },
      "source": [
        "### Dataset Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xY5IEN7G-Nns"
      },
      "outputs": [],
      "source": [
        "split = 0.2\n",
        "full_output = False\n",
        "use_trig = False\n",
        "noise = False\n",
        "noise_percent = 10\n",
        "neural_network = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zevN56PhZrM"
      },
      "outputs": [],
      "source": [
        "def add_noise_to_dataset(dataframe, noise_percentage):\n",
        "    random_generator = np.random.default_rng(seed)\n",
        "    noise_standard_deviation = 0.01\n",
        "    noisy_dataframe = dataframe.copy()\n",
        "\n",
        "    number_of_noisy_rows = int((noise_percentage / 100) * len(noisy_dataframe))\n",
        "\n",
        "    indices_for_noisy_rows = random_generator.choice(\n",
        "        noisy_dataframe.index, size=number_of_noisy_rows, replace=False\n",
        "    )\n",
        "\n",
        "    for column in noisy_dataframe.select_dtypes(include=np.number).columns:\n",
        "        noisy_dataframe.loc[indices_for_noisy_rows, column] += random_generator.normal(\n",
        "            0, noise_standard_deviation, size=number_of_noisy_rows\n",
        "        )\n",
        "\n",
        "    noisy_dataframe = noisy_dataframe.round(3)\n",
        "    return noisy_dataframe\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L16hrrV24sd"
      },
      "source": [
        "### Dataset R2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGeYVxMSh3oq"
      },
      "outputs": [],
      "source": [
        "df2 = pd.read_csv(d2_file, delimiter=';')\n",
        "if noise:\n",
        "  df2 = generate_dataset(df2, noise_percent)\n",
        "\n",
        "if full_output:\n",
        "  df_target2 = df2[[' ee_x', ' ee_y', ' ee_qw', ' ee_qz']].copy()\n",
        "  df_target2.columns = ['ee_x', 'ee_y', 'ee_qw', 'ee_qz']\n",
        "else:\n",
        "  df_target2 = df2[[' ee_x', ' ee_y']].copy()\n",
        "  df_target2.columns = ['ee_x', 'ee_y']\n",
        "df2.drop([' cos(j0)', ' cos(j1)', ' sin(j0)', ' sin(j1)', ' ee_x', ' ee_y', ' ee_qw', ' ee_qz'], axis=1, inplace=True)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(df2, df_target2, test_size=split, random_state=seed)\n",
        "\n",
        "if neural_network:\n",
        "  X_train = tf.convert_to_tensor(X_train2, dtype=tf.float32)\n",
        "  y_train = tf.convert_to_tensor(y_train2, dtype=tf.float32)\n",
        "\n",
        "  X_test = tf.convert_to_tensor(X_test2, dtype=tf.float32)\n",
        "  y_test = tf.convert_to_tensor(y_test2, dtype=tf.float32)\n",
        "else:\n",
        "  X_train = np.array(X_train2)\n",
        "  y_train = np.array(y_train2)\n",
        "\n",
        "  X_test = np.array(X_test2)\n",
        "  y_test = np.array(y_test2)\n",
        "dataset = 'r2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b36apcLGGClN"
      },
      "source": [
        "### Dataset R3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTmfj0WJGFrK"
      },
      "outputs": [],
      "source": [
        "df3 = pd.read_csv(d3_file, delimiter=';')\n",
        "if noise:\n",
        "  df3 = generate_dataset(df3, noise_percent)\n",
        "\n",
        "if full_output:\n",
        "  df_target3 = df3[[' ee_x', ' ee_y', ' ee_qw', ' ee_qz']].copy()\n",
        "  df_target3.columns = ['ee_x', 'ee_y', 'ee_qw', 'ee_qz']\n",
        "else:\n",
        "  df_target3 = df3[[' ee_x', ' ee_y']].copy()\n",
        "  df_target3.columns = ['ee_x', 'ee_y']\n",
        "df3.drop([' cos(j0)', ' cos(j1)', ' cos(j2)', ' sin(j0)', ' sin(j1)',' sin(j2)',' ee_x', ' ee_y', ' ee_qw', ' ee_qz'], axis=1, inplace=True)\n",
        "\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(df3, df_target3, test_size=split, random_state=seed)\n",
        "\n",
        "if neural_network:\n",
        "  X_train = tf.convert_to_tensor(X_train3, dtype=tf.float32)\n",
        "  y_train = tf.convert_to_tensor(y_train3, dtype=tf.float32)\n",
        "\n",
        "  X_test = tf.convert_to_tensor(X_test3, dtype=tf.float32)\n",
        "  y_test = tf.convert_to_tensor(y_test3, dtype=tf.float32)\n",
        "else:\n",
        "  X_train = np.array(X_train3)\n",
        "  y_train = np.array(y_train3)\n",
        "\n",
        "  X_test = np.array(X_test3)\n",
        "  y_test = np.array(y_test3)\n",
        "\n",
        "dataset = 'r3'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwRH3-UYGQwN"
      },
      "source": [
        "### Dataset R5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2koaJkfTGUsq"
      },
      "outputs": [],
      "source": [
        "df5 = pd.read_csv(d5_file, delimiter=';')\n",
        "\n",
        "if noise:\n",
        "    df5 = generate_dataset(df5, noise_percent)\n",
        "\n",
        "df_target5 = df5[[' ee_x', ' ee_y', ' ee_z']].copy()\n",
        "\n",
        "df5 = df5[['j0', ' j1', ' j2', ' j3', ' j4']].copy()\n",
        "\n",
        "df_target5.columns = ['ee_x', 'ee_y', 'ee_z']\n",
        "\n",
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(df5, df_target5, test_size=split, random_state=seed)\n",
        "\n",
        "if neural_network:\n",
        "    X_train = tf.convert_to_tensor(X_train5, dtype=tf.float32)\n",
        "    y_train = tf.convert_to_tensor(y_train5, dtype=tf.float32)\n",
        "\n",
        "    X_test = tf.convert_to_tensor(X_test5, dtype=tf.float32)\n",
        "    y_test = tf.convert_to_tensor(y_test5, dtype=tf.float32)\n",
        "else:\n",
        "    X_train = np.array(X_train5)\n",
        "    y_train = np.array(y_train5)\n",
        "\n",
        "    X_test = np.array(X_test5)\n",
        "    y_test = np.array(y_test5)\n",
        "\n",
        "dataset = 'r5'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "209B6i0y3Bq5"
      },
      "source": [
        "## Forward Kinematics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXeKMVZ-5116"
      },
      "source": [
        "#### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXreKg6M50Og",
        "outputId": "d7fb64f7-76ed-4980-fda8-29b465715279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r5\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.00025\n",
        "loss = 'mse'\n",
        "metrics = ['mae']\n",
        "epochs = 200\n",
        "batch_size = 32\n",
        "opt = 'adam'\n",
        "split = 0.2\n",
        "hidden_layers = 5\n",
        "units_per_layer = 80\n",
        "dropout = 0\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Mde9p_R5sTZ"
      },
      "outputs": [],
      "source": [
        "if opt == 'adam':\n",
        "  optimizer = Adam(learning_rate=learning_rate)\n",
        "elif opt == 'sgd':\n",
        "  optimizer = SGD(learning_rate=learning_rate)\n",
        "elif opt == 'RMSprop':\n",
        "  optimizer = RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = y_train.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIGtMr-V3T3l"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QENI_zUhyibR"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(InputLayer(shape=(input_dim,)))\n",
        "r5_units = [224, 96, 192, 160, 96, 32, 160, 160]\n",
        "if use_trig:\n",
        "  if dataset == 'r2':\n",
        "    model.add(Lambda(lambda inputs: tf.stack([inputs[:, 0], inputs[:, 1],\n",
        "                                                sin(inputs[:, 0]), sin(inputs[:, 1]),\n",
        "                                                cos(inputs[:, 0]), cos(inputs[:, 1])], axis=1)))\n",
        "  elif dataset == 'r3':\n",
        "    model.add(Lambda(lambda inputs: tf.stack([inputs[:, 0], inputs[:, 1], inputs[:, 2],\n",
        "                                                sin(inputs[:, 0]), sin(inputs[:, 1]), sin(inputs[:, 2]),\n",
        "                                                cos(inputs[:, 0]), cos(inputs[:, 1]), cos(inputs[:, 2])], axis=1)))\n",
        "  elif dataset == 'r5':\n",
        "      model.add(Lambda(lambda inputs: tf.stack([inputs[:, 0], inputs[:, 1], inputs[:, 2], inputs[:, 3], inputs[:, 4],\n",
        "                                                  sin(inputs[:, 0]), sin(inputs[:, 1]), sin(inputs[:, 2]), sin(inputs[:, 3]), sin(inputs[:, 4]),\n",
        "                                                  cos(inputs[:, 0]), cos(inputs[:, 1]), cos(inputs[:, 2]), cos(inputs[:, 3]), cos(inputs[:, 4])], axis=1)))\n",
        "\n",
        "if dataset == 'r5':\n",
        "  for units in r5_units:\n",
        "    model.add(Dense(units=units, activation='relu'))\n",
        "    hidden_layers = len(r5_units)\n",
        "else:\n",
        "  for _ in range(hidden_layers):\n",
        "    model.add(Dense(units_per_layer, activation='relu'))\n",
        "\n",
        "if dropout > 0:\n",
        "  model.add(Dropout(dropout))\n",
        "model.add(Dense(output_dim))\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=loss,\n",
        "              metrics=metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eENDR7E3W_a"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFoZeIPQ9ut5"
      },
      "outputs": [],
      "source": [
        "start_log_wandb(learning_rate, type(model).__name__, dataset, epochs, opt, loss, metrics, batch_size, split, hidden_layers, units_per_layer, dropout, full_output, noise, noise_percent)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    WandbCallback(\n",
        "        monitor=\"val_mean_squared_error\",\n",
        "        mode=\"min\",\n",
        "        log_weights=False,\n",
        "        log_gradients=False,\n",
        "        save_graph=False\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "yDnJfHVTuJq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWb9fsjSy4jg"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1, callbacks=callbacks)\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test MAE: {test_mae:.4f}')\n",
        "wandb.log({\n",
        "    \"test_loss\": test_loss,\n",
        "    \"test_mae\": test_mae\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Jacobian comparison"
      ],
      "metadata": {
        "id": "vbd9kWgUC8VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def FK_Jacobian(model, x):\n",
        "    x = tf.reshape(x, (1, x.shape[0]))\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        tape.watch(x)\n",
        "        y = model(x)\n",
        "    jacobian_matrix = tape.jacobian(y, x)\n",
        "    return jacobian_matrix"
      ],
      "metadata": {
        "id": "lCHBMprIK7zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analytical_jacobian_r2(theta):\n",
        "    theta = theta.flatten()\n",
        "    l1 = 0.1\n",
        "    l2 = 0.1\n",
        "    jacobian = np.zeros((2, 2))\n",
        "    jacobian[0, 0] = -l1 * np.sin(theta[0]) - l2 * np.sin(theta[0] + theta[1])\n",
        "    jacobian[0, 1] = -l2 * np.sin(theta[0] + theta[1])\n",
        "    jacobian[1, 0] = l1 * np.cos(theta[0]) + l2 * np.cos(theta[0] + theta[1])\n",
        "    jacobian[1, 1] = l2 * np.cos(theta[0] + theta[1])\n",
        "    return jacobian\n",
        "\n",
        "def analytical_jacobian_r3(theta):\n",
        "\n",
        "    theta = theta.flatten()\n",
        "    if len(theta) < 3:\n",
        "        raise ValueError(\"Input theta must have at least 3 elements.\")\n",
        "\n",
        "    l1 = 0.1\n",
        "    l2 = 0.1\n",
        "    l3 = 0.1\n",
        "    jacobian = np.zeros((2, 3))\n",
        "\n",
        "    jacobian[0, 0] = -l1 * np.sin(theta[0]) - l2 * np.sin(theta[0] + theta[1]) - l3 * np.sin(theta[0] + theta[1] + theta[2])\n",
        "    jacobian[0, 1] = -l2 * np.sin(theta[0] + theta[1]) - l3 * np.sin(theta[0] + theta[1] + theta[2])\n",
        "    jacobian[0, 2] = -l3 * np.sin(theta[0] + theta[1] + theta[2])\n",
        "\n",
        "    jacobian[1, 0] = l1 * np.cos(theta[0]) + l2 * np.cos(theta[0] + theta[1]) + l3 * np.cos(theta[0] + theta[1] + theta[2])\n",
        "    jacobian[1, 1] = l2 * np.cos(theta[0] + theta[1]) + l3 * np.cos(theta[0] + theta[1] + theta[2])\n",
        "    jacobian[1, 2] = l3 * np.cos(theta[0] + theta[1] + theta[2])\n",
        "\n",
        "    return jacobian\n",
        "\n",
        "def analytical_jacobian_r5(theta):\n",
        "    if len(theta) != 5:\n",
        "        raise ValueError(\"Input theta must have 5 elements.\")\n",
        "\n",
        "    a = [0.0, 0.45, 0.11, 0.11, 0.115]\n",
        "\n",
        "    def transformation_matrix(theta, alpha, a, d):\n",
        "        return np.array([\n",
        "            [np.cos(theta), -np.sin(theta) * np.cos(alpha), np.sin(theta) * np.sin(alpha), a * np.cos(theta)],\n",
        "            [np.sin(theta), np.cos(theta) * np.cos(alpha), -np.cos(theta) * np.sin(alpha), a * np.sin(theta)],\n",
        "            [0, np.sin(alpha), np.cos(alpha), d],\n",
        "            [0, 0, 0, 1]\n",
        "        ])\n",
        "\n",
        "    T01 = np.array([\n",
        "        [np.sin(theta[0]), 0, np.cos(theta[0]), a[0] * np.sin(theta[0])],\n",
        "        [np.cos(theta[0]), 0, np.sin(theta[0]), -a[0] * np.cos(theta[0])],\n",
        "        [0, -1, 0, 0],\n",
        "        [0, 0, 0, 1]\n",
        "    ])\n",
        "\n",
        "    T12 = np.array([\n",
        "        [np.sin(theta[1]), np.cos(theta[1]), 0, a[1] * np.sin(theta[1])],\n",
        "        [-np.cos(theta[1]), np.sin(theta[1]), 0, -a[1] * np.cos(theta[1])],\n",
        "        [0, 0, 1, 0],\n",
        "        [0, 0, 0, 1]\n",
        "    ])\n",
        "\n",
        "    T23 = np.array([\n",
        "        [np.cos(theta[2]), -np.sin(theta[2]), 0, a[2] * np.cos(theta[2])],\n",
        "        [np.sin(theta[2]), np.cos(theta[2]), 0, a[2] * np.sin(theta[2])],\n",
        "        [0, 0, 1, 0],\n",
        "        [0, 0, 0, 1]\n",
        "    ])\n",
        "\n",
        "    T34 = np.array([\n",
        "        [np.cos(theta[3]), 0, -np.sin(theta[3]), a[3] * np.cos(theta[3])],\n",
        "        [np.sin(theta[3]), 0, np.cos(theta[3]), a[3] * np.sin(theta[3])],\n",
        "        [0, -1, 0, 0],\n",
        "        [0, 0, 0, 1]\n",
        "    ])\n",
        "\n",
        "    T45 = np.array([\n",
        "        [np.cos(theta[4]), np.sin(theta[4]), 0, a[4] * np.cos(theta[4])],\n",
        "        [np.sin(theta[4]), -np.cos(theta[4]), 0, a[4] * np.sin(theta[4])],\n",
        "        [0, 0, -1, 0],\n",
        "        [0, 0, 0, 1]\n",
        "    ])\n",
        "\n",
        "    T05 = T01 @ T12 @ T23 @ T34 @ T45\n",
        "    px, py, pz = T05[0, 3], T05[1, 3], T05[2, 3]\n",
        "\n",
        "    J_pos = np.zeros((3, 5))\n",
        "    for i in range(5):\n",
        "        theta_copy = theta.copy()\n",
        "        d_theta = 1e-5\n",
        "        theta_copy[i] += d_theta\n",
        "\n",
        "        T01_new = np.array([\n",
        "            [np.sin(theta_copy[0]), 0, np.cos(theta_copy[0]), a[0] * np.sin(theta_copy[0])],\n",
        "            [np.cos(theta_copy[0]), 0, np.sin(theta_copy[0]), -a[0] * np.cos(theta_copy[0])],\n",
        "            [0, -1, 0, 0],\n",
        "            [0, 0, 0, 1]\n",
        "        ])\n",
        "\n",
        "        T12_new = np.array([\n",
        "            [np.sin(theta_copy[1]), np.cos(theta_copy[1]), 0, a[1] * np.sin(theta_copy[1])],\n",
        "            [-np.cos(theta_copy[1]), np.sin(theta_copy[1]), 0, -a[1] * np.cos(theta_copy[1])],\n",
        "            [0, 0, 1, 0],\n",
        "            [0, 0, 0, 1]\n",
        "        ])\n",
        "\n",
        "        T23_new = np.array([\n",
        "            [np.cos(theta_copy[2]), -np.sin(theta_copy[2]), 0, a[2] * np.cos(theta_copy[2])],\n",
        "            [np.sin(theta_copy[2]), np.cos(theta_copy[2]), 0, a[2] * np.sin(theta_copy[2])],\n",
        "            [0, 0, 1, 0],\n",
        "            [0, 0, 0, 1]\n",
        "        ])\n",
        "\n",
        "        T34_new = np.array([\n",
        "            [np.cos(theta_copy[3]), 0, -np.sin(theta_copy[3]), a[3] * np.cos(theta_copy[3])],\n",
        "            [np.sin(theta_copy[3]), 0, np.cos(theta_copy[3]), a[3] * np.sin(theta_copy[3])],\n",
        "            [0, -1, 0, 0],\n",
        "            [0, 0, 0, 1]\n",
        "        ])\n",
        "\n",
        "        T45_new = np.array([\n",
        "            [np.cos(theta_copy[4]), np.sin(theta_copy[4]), 0, a[4] * np.cos(theta_copy[4])],\n",
        "            [np.sin(theta_copy[4]), -np.cos(theta_copy[4]), 0, a[4] * np.sin(theta_copy[4])],\n",
        "            [0, 0, -1, 0],\n",
        "            [0, 0, 0, 1]\n",
        "        ])\n",
        "\n",
        "        T05_new = T01_new @ T12_new @ T23_new @ T34_new @ T45_new\n",
        "        px_new, py_new, pz_new = T05_new[0, 3], T05_new[1, 3], T05_new[2, 3]\n",
        "\n",
        "        J_pos[0, i] = (px_new - px) / d_theta\n",
        "        J_pos[1, i] = (py_new - py) / d_theta\n",
        "        J_pos[2, i] = (pz_new - pz) / d_theta\n",
        "\n",
        "    return J_pos"
      ],
      "metadata": {
        "id": "-CLULOP6cdrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3jr0fV4YRX1"
      },
      "outputs": [],
      "source": [
        "X = np.array(X_train[1])\n",
        "X_model = inputs = tf.expand_dims(tf.convert_to_tensor(X, dtype=tf.float32), axis=0)\n",
        "\n",
        "if dataset == 'r2':\n",
        "  jacobian_matrix_true = analytical_jacobian_r2(X)\n",
        "  jacobian_matrix_true = tf.squeeze(jacobian_matrix_true).numpy()\n",
        "elif dataset == 'r3':\n",
        "  jacobian_matrix_true = analytical_jacobian_r3(X)\n",
        "  jacobian_matrix_true = tf.squeeze(jacobian_matrix_true).numpy()\n",
        "elif dataset == 'r5':\n",
        "  jacobian_matrix_true = analytical_jacobian_r5(X)\n",
        "  jacobian_matrix_true = tf.squeeze(jacobian_matrix_true).numpy()\n",
        "else:\n",
        "  raise ValueError(f\"Unknown dataset: {dataset}\")\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "jacobian_matrix_model = FK_Jacobian(model, X_model)\n",
        "jacobian_matrix_model = tf.squeeze(jacobian_matrix_model).numpy()\n",
        "\n",
        "print(jacobian_matrix_true)\n",
        "print(jacobian_matrix_model)\n",
        "mse_jacobian = np.mean((jacobian_matrix_true - jacobian_matrix_model) ** 2)\n",
        "print(mse_jacobian)\n",
        "\n",
        "wandb.log({\"mse_jacobian\": mse_jacobian})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siF-17VuuQb2"
      },
      "outputs": [],
      "source": [
        "model_name = wandb.run.name + '_' +dataset + '_' + type(model).__name__ + str(hidden_layers)  + '_' + loss + '_' + opt + '_' + str(learning_rate)\n",
        "model_file = model_name + '.h5'\n",
        "print(model_file)\n",
        "model.save(model_file)\n",
        "\n",
        "wandb.save(model_file)\n",
        "\n",
        "artifact = wandb.Artifact(model_name, type=\"model\")\n",
        "artifact.add_file(model_file)\n",
        "wandb.log_artifact(artifact)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZAYRLrcYM8z"
      },
      "source": [
        "#### Hyperparameter search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f0dJGyslKy0"
      },
      "outputs": [],
      "source": [
        "input_dim = X_train.shape[1]\n",
        "output_dim = y_train.shape[1]\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(InputLayer(shape=(input_dim,)))\n",
        "    for i in range(hp.Int('hidden_layers', 1, 10)):\n",
        "        units = hp.Int(f'units_{i}', min_value=32, max_value=256, step=32)\n",
        "        model.add(Dense(units=units, activation='relu'))\n",
        "\n",
        "    model.add(Dense(output_dim))\n",
        "\n",
        "    optimizer_choice = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop'])\n",
        "\n",
        "    if optimizer_choice == 'adam':\n",
        "        optimizer = Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='log'))\n",
        "    elif optimizer_choice == 'sgd':\n",
        "        optimizer = SGD(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='log'))\n",
        "    elif optimizer_choice == 'rmsprop':\n",
        "        optimizer = RMSprop(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='log'))\n",
        "\n",
        "    loss_choice = hp.Choice('loss', values=['mse', 'mae', 'huber', 'logcosh'])\n",
        "\n",
        "    if loss_choice == 'mse':\n",
        "        loss = MeanSquaredError()\n",
        "    elif loss_choice == 'mae':\n",
        "        loss = MeanAbsoluteError()\n",
        "    elif loss_choice == 'huber':\n",
        "        loss = Huber()\n",
        "    elif loss_choice == 'logcosh':\n",
        "        loss = LogCosh()\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=loss,\n",
        "                  metrics=[MeanAbsoluteError()])\n",
        "\n",
        "    return model\n",
        "\n",
        "tuner = Hyperband(\n",
        "    build_model,\n",
        "    objective='val_mean_absolute_error',\n",
        "    max_epochs=50,\n",
        "    factor=3,\n",
        "    directory=base_path + 'hyperband_dir',\n",
        "    project_name='ann_hyperband_tuning' + dataset,\n",
        "    executions_per_trial=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(X_train, y_train, epochs=50, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "F9yGz-0yCXCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gP3sUIBe5KGw"
      },
      "outputs": [],
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "hidden_units = []\n",
        "print(\"Best hyperparameters:\")\n",
        "print(f\"Hidden Layers: {best_hps.get('hidden_layers')}\")\n",
        "for i in range(10):\n",
        "    units = best_hps.get(f'units_{i}')\n",
        "    if i < 8:\n",
        "        hidden_units.append(units)\n",
        "print(f\"Learning Rate: {best_hps.get('learning_rate')}\")\n",
        "print(f\"Optimizer: {best_hps.get('optimizer')}\")\n",
        "print(f\"Loss: {best_hps.get('loss')}\")\n",
        "print(f\"Hidden units per layer: {hidden_units}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tuner.get_best_models(num_models=1)[0]\n",
        "model.summary()\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test MAE: {test_mae:.4f}')\n",
        "print(f'Test Loss: {test_loss:.6f}')"
      ],
      "metadata": {
        "id": "y2BH-savbQWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M6XhupK41zJ"
      },
      "outputs": [],
      "source": [
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_model.save(f\"{base_path}/models/{dataset}_tuned.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2lHCrMMkgg0"
      },
      "source": [
        "#### SVR\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svr = MultiOutputRegressor(SVR(kernel='rbf', gamma='auto', epsilon=0.01, C=10))"
      ],
      "metadata": {
        "id": "2jydW6iClIi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svr.fit(X_train, y_train)\n",
        "y_pred = svr.predict(X_test)\n",
        "\n",
        "mae_x = mean_absolute_error(y_test[:, 0], y_pred[:, 0])\n",
        "mae_y = mean_absolute_error(y_test[:, 1], y_pred[:, 1])\n",
        "total_mae = (mae_x + mae_y) / 2\n",
        "print(f\"Best MAE for x prediction: {mae_x}\")\n",
        "print(f\"Best MAE for y prediction: {mae_y}\")\n",
        "print(f\"Best Total MAE: {total_mae}\")"
      ],
      "metadata": {
        "id": "GyN9FMy7lk8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a26e963-3cb5-4070-94c6-3de0bcd6e162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best MAE for x prediction: 0.005642613406925205\n",
            "Best MAE for y prediction: 0.005780416908938051\n",
            "Best Total MAE: 0.005711515157931628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJK6rNdplf4M"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    \"estimator__C\": [0.1, 1, 10, 100],\n",
        "    \"estimator__epsilon\": [0.01, 0.1, 0.2, 0.5],\n",
        "    \"estimator__gamma\": [\"scale\", \"auto\"]\n",
        "}\n",
        "\n",
        "svr = MultiOutputRegressor(SVR(kernel='rbf'))\n",
        "grid_search = RandomizedSearchCV(svr, param_grid, n_iter=10, cv=3, n_jobs=-1, verbose=3)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "mae_x = mean_absolute_error(y_test[:, 0], y_pred[:, 0])\n",
        "mae_y = mean_absolute_error(y_test[:, 1], y_pred[:, 1])\n",
        "total_mae = (mae_x + mae_y) / 2\n",
        "\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "print(f\"Best MAE for x prediction: {mae_x}\")\n",
        "print(f\"Best MAE for y prediction: {mae_y}\")\n",
        "print(f\"Best Total MAE: {total_mae}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model testing"
      ],
      "metadata": {
        "id": "2Hi8SU1oCxHu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FmVfmoadQV8"
      },
      "outputs": [],
      "source": [
        "model = load_model(base_path + 'models/r5_tuned.h5')\n",
        "model.summary()\n",
        "\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test MAE: {test_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init(project=\"MLH1\")\n",
        "artifact = run.use_artifact('sapienza-ceriotti-hw/MLH1/charmed-wave-40_r5_Sequential8_logCosH_adam_0.00025:v0', type='model')\n",
        "artifact_dir = artifact.download()"
      ],
      "metadata": {
        "id": "wrdK_29pj_QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(artifact_dir + '/charmed-wave-40_r5_Sequential8_logCosH_adam_0.00025.h5', compile=False)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "HcP5ftGKlTsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Erase the weights\n",
        "model_config = model.to_json()\n",
        "\n",
        "new_model = tf.keras.models.model_from_json(model_config)\n",
        "model = new_model"
      ],
      "metadata": {
        "id": "IstlyS4Xx2i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = 'adam'\n",
        "loss = 'mse'\n",
        "model.compile(optimizer=opt,\n",
        "                  loss=loss,\n",
        "                  metrics=[MeanAbsoluteError()])"
      ],
      "metadata": {
        "id": "Q3UIJQbzXIX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test MAE: {test_mae:.4f}')\n",
        "print(f\"Test Loss: {test_loss:.6f}\")"
      ],
      "metadata": {
        "id": "B0bCDugclyjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = model.optimizer\n",
        "print(f\"Optimizer: {optimizer}\")\n",
        "\n",
        "learning_rate = optimizer.learning_rate\n",
        "print(f\"Learning Rate: {learning_rate.numpy()}\")\n",
        "\n",
        "loss = model.loss\n",
        "print(f\"Loss: {loss}\")\n",
        "\n",
        "metrics = model.metrics_names\n",
        "print(f\"Metrics: {metrics}\")"
      ],
      "metadata": {
        "id": "dM8hFudIm3dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inverse Kinematics"
      ],
      "metadata": {
        "id": "KNH4u0IEy8_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inverse_kinematics_r2(model, x_d, initial_guess=np.array([-0.5, 0.5]),\n",
        "                                 max_iters=500, tolerance=1e-5, damping=0.01):\n",
        "    theta = initial_guess\n",
        "\n",
        "    for i in range(max_iters):\n",
        "\n",
        "        x_pred = model(tf.reshape(theta, (1, 2)))\n",
        "        x_pred = x_pred.numpy().flatten()\n",
        "\n",
        "        error = np.array([x_pred[0] - x_d[0], x_pred[1] - x_d[1]])\n",
        "\n",
        "        if np.linalg.norm(error) < tolerance:\n",
        "            print(f\"Converged after {i+1} iterations: {theta}\")\n",
        "            return theta\n",
        "\n",
        "        jacobian_matrix = FK_Jacobian(model, theta)\n",
        "        jacobian_matrix = jacobian_matrix.numpy().reshape(2, 2)\n",
        "\n",
        "        JTJ = jacobian_matrix.T @ jacobian_matrix\n",
        "        identity = damping * np.eye(JTJ.shape[0])\n",
        "        delta_theta = np.linalg.solve(JTJ + identity, -jacobian_matrix.T @ error)\n",
        "\n",
        "        theta = theta + delta_theta\n",
        "        theta = (theta + np.pi) % (2 * np.pi) - np.pi\n",
        "\n",
        "    print(\"Did not converge within the maximum number of iterations\")\n",
        "    return theta\n",
        "\n",
        "def inverse_kinematics_r3(model, x_d, initial_guess=np.array([-0.5, 0.5, 0.5]),\n",
        "                                 max_iters=500, tolerance=1e-5, damping=0.01):\n",
        "    theta = initial_guess\n",
        "\n",
        "    for i in range(max_iters):\n",
        "\n",
        "        x_pred = model(tf.reshape(theta, (1, 3)))\n",
        "        x_pred = x_pred.numpy().flatten()\n",
        "\n",
        "        error = np.array([x_pred[0] - x_d[0], x_pred[1] - x_d[1]])\n",
        "\n",
        "        if np.linalg.norm(error) < tolerance:\n",
        "            print(f\"Converged after {i+1} iterations: {theta}\")\n",
        "            return theta\n",
        "\n",
        "        jacobian_matrix = FK_Jacobian(model, theta)\n",
        "        jacobian_matrix = jacobian_matrix.numpy().reshape(2, 3)\n",
        "\n",
        "        JTJ = jacobian_matrix.T @ jacobian_matrix\n",
        "        identity = damping * np.eye(JTJ.shape[0])\n",
        "        delta_theta = np.linalg.solve(JTJ + identity, -jacobian_matrix.T @ error)\n",
        "\n",
        "        theta = theta + delta_theta\n",
        "        theta = (theta + np.pi) % (2 * np.pi) - np.pi\n",
        "\n",
        "    print(\"Did not converge within the maximum number of iterations\")\n",
        "    return theta\n",
        "\n",
        "\n",
        "def inverse_kinematics_r5(model, x_d, initial_guess=np.array([-0.5, 0.5, 0.5, 0.5, 0.5]),\n",
        "                                 max_iters=500, tolerance=1e-5, damping=0.01):\n",
        "    theta = initial_guess\n",
        "\n",
        "    for i in range(max_iters):\n",
        "\n",
        "        x_pred = model(tf.reshape(theta, (1, 5)))\n",
        "        x_pred = x_pred.numpy().flatten()\n",
        "\n",
        "        error = np.array([x_pred[0] - x_d[0], x_pred[1] - x_d[1], x_pred[2] - x_d[2]])\n",
        "\n",
        "        if np.linalg.norm(error) < tolerance:\n",
        "            print(f\"Converged after {i+1} iterations: {theta}\")\n",
        "            return theta\n",
        "\n",
        "        jacobian_matrix = FK_Jacobian(model, theta)\n",
        "        jacobian_matrix = jacobian_matrix.numpy().reshape(3, 5)\n",
        "\n",
        "        JTJ = jacobian_matrix.T @ jacobian_matrix\n",
        "        identity = damping * np.eye(JTJ.shape[0])\n",
        "        delta_theta = np.linalg.solve(JTJ + identity, -jacobian_matrix.T @ error)\n",
        "\n",
        "        theta = theta + delta_theta\n",
        "        theta = (theta + np.pi) % (2 * np.pi) - np.pi\n",
        "\n",
        "    print(\"Did not converge within the maximum number of iterations\")\n",
        "    return theta\n",
        "\n"
      ],
      "metadata": {
        "id": "CHH3NSVgzCmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_idx = np.random.randint(len(X_test))\n",
        "x_d = y_test[random_idx].numpy()\n",
        "true_joint_angles = X_test[random_idx].numpy()\n",
        "if dataset == 'r2':\n",
        "  theta_solution = inverse_kinematics_r2(model, x_d)\n",
        "if dataset == 'r3':\n",
        "  theta_solution = inverse_kinematics_r3(model, x_d)\n",
        "if dataset == 'r5':\n",
        "  theta_solution = inverse_kinematics_r5(model, x_d)\n",
        "\n",
        "theta_solution = (theta_solution + np.pi) % (2 * np.pi) - np.pi\n",
        "true_joint_angles = (true_joint_angles + np.pi) % (2 * np.pi) - np.pi\n",
        "\n",
        "predicted_position = model(tf.reshape(theta_solution, (1, theta_solution.shape[0]))).numpy().flatten()\n",
        "position_mae = mean_absolute_error(x_d, predicted_position)\n",
        "\n",
        "print(f\"Solution for the joint angles: {theta_solution}\")\n",
        "print(f\"True joint angles: {true_joint_angles}\")\n",
        "print(f\"Position-based MAE between predicted and true positions: {position_mae:.6f}\")"
      ],
      "metadata": {
        "id": "0avrWkS91a1y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "yrzCvNCx2zkh",
        "-FyNY9jB22fJ",
        "kk-vO8z4Didx",
        "8L16hrrV24sd",
        "b36apcLGGClN",
        "MwRH3-UYGQwN",
        "HXeKMVZ-5116"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}